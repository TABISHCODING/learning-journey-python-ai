
---

## ğŸ§  Module: Machine Learning Foundations

### ğŸ¯ Objective

Learn the **core concepts of Machine Learning (ML)** â€” what it is, how it works, and explore the main categories: **Supervised, Unsupervised, and Reinforcement Learning**, with a focus on **Linear Regression** as the first supervised learning model.

---

## ğŸ§© 1. Introduction to Machine Learning

### ğŸ“˜ What is Machine Learning?

Machine Learning (ML) is a **subset of Artificial Intelligence (AI)** that enables computers to **learn from data** and make predictions or decisions **without explicit programming**.

> ğŸ§  ML = Algorithms + Data â†’ Model â†’ Predictions

### âš™ï¸ How It Works

1. **Input (Features):** The data or attributes used to make predictions (e.g., size of house).
2. **Output (Label):** The expected result (e.g., house price).
3. **Training:** The model learns the relationship between features and labels.
4. **Inference:** The trained model predicts output for new data.

### ğŸ’¡ Everyday Examples

| Example                    | Description                                                 |
| -------------------------- | ----------------------------------------------------------- |
| ğŸ›’ Product Recommendations | Amazon/Flipkart suggest products based on past purchases    |
| ğŸ¬ Movie Recommendations   | Netflix suggests movies based on viewing history            |
| ğŸ“§ Spam Detection          | Email filters classify spam using content & metadata        |
| ğŸš— Self-Driving Cars       | ML helps navigate roads, detect objects, and make decisions |

---

## ğŸ§® 2. How Machine Learning Works (Example: Cat vs Dog)

### ğŸ± Example: Classifying Animals

1. Provide **input data (features)** â€” e.g., color, texture, eye color.
2. Provide **labels** â€” e.g., *cat* or *dog*.
3. Train the model using these examples.
4. Once trained, the model can predict labels for **new unseen data**.

### ğŸ§  Key Terms

| Term              | Description                                            |
| ----------------- | ------------------------------------------------------ |
| **Training Data** | Data with both input features and labels               |
| **Model**         | Mathematical representation that maps inputs â†’ outputs |
| **Inference**     | Using the trained model to predict for new inputs      |
| **Label**         | Target variable or desired output                      |

---

## ğŸ”¢ 3. Types of Machine Learning

| Type                       | Uses Labeled Data? | Purpose                                | Common Use Cases                                  |
| -------------------------- | ------------------ | -------------------------------------- | ------------------------------------------------- |
| **Supervised Learning**    | âœ… Yes              | Learn mapping between inputs & outputs | Disease detection, spam filtering, credit scoring |
| **Unsupervised Learning**  | âŒ No               | Discover patterns & clusters           | Customer segmentation, fraud detection            |
| **Reinforcement Learning** | ğŸ§© Feedback-based  | Learn via rewards & penalties          | Robots, autonomous cars, games                    |

---

## ğŸ©º 4. Real-World Examples

### ğŸ§­ Supervised Learning

* ğŸ§¬ Disease detection (predict illness from patient data)
* ğŸŒ¦ Weather forecasting
* ğŸ’¹ Stock price prediction
* ğŸ§¾ Credit scoring

### ğŸ§­ Unsupervised Learning

* ğŸ’³ Fraud detection
* ğŸ§ Customer segmentation
* ğŸ“Š Outlier detection
* ğŸ¯ Targeted marketing

### ğŸ§­ Reinforcement Learning

* ğŸ¤– Automated robots
* ğŸš— Autonomous driving cars
* ğŸ® Game playing (e.g., chess, Go)

---

## ğŸ§© 5. Supervised Learning (In-Depth)

### ğŸ“˜ Definition

Supervised learning trains a model using **labeled data** â€” meaning both **inputs (features)** and **outputs (labels)** are known.
The goal is to learn the **mapping** between inputs and outputs.

> ğŸ“ Analogy: Like a teacher guiding a student with correct answers during training.

---

## ğŸ¡ 6. Example: Predicting House Prices

### ğŸ“ˆ Problem Statement

Predict the **price of a house** based on its **size (sq. ft.)**.

| Feature (Input)      | Label (Output)  |
| -------------------- | --------------- |
| House size (sq. ft.) | House price ($) |

### ğŸ§© Relationship

* As house size increases â†’ price tends to increase.
* Represent this relationship as a **line** on a scatter plot.

---

## ğŸ“ 7. Linear Regression (Supervised Learning Model)

### ğŸ“˜ Concept

Linear Regression finds a **best-fit straight line** that describes the relationship between **input (x)** and **output (y)**.

**Equation:**
[
f(x) = w \cdot x + b
]
Where:

* ( w ): **Slope (weight)** â†’ how much output changes with input
* ( b ): **Intercept (bias)** â†’ where the line crosses the y-axis

---

### âš™ï¸ How It Works

1. **Model Training:**

   * The algorithm adjusts ( w ) and ( b ) so that predictions are close to actual values.
2. **Error (Loss):**

   * Difference between predicted and actual output.
3. **Loss Function:**

   * Squared error = ((y_{pred} - y_{actual})^2)
4. **Optimization:**

   * Iteratively update ( w ) and ( b ) to minimize the loss (using gradient descent).

---

### ğŸ“Š Visual Understanding

* **Slope (w):** Controls tilt of the line.
* **Bias (b):** Moves the line up or down.
* **Goal:** Find the line that passes as close as possible to all data points (minimizing total error).

---

## ğŸ§® 8. Model Usage (Prediction Phase)

Once the model is trained:

1. Give it new input (e.g., house size = 1100 sq. ft.)
2. Model uses learned function ( f(x) ) to **predict price**.

âœ… Example:
If the line equation is ( f(x) = 150x + 10000 ),
then for ( x = 1100 ):
Predicted price = ( 150Ã—1100 + 10000 = $175,000 )

---

## ğŸ§¾ 9. Summary

| Concept                 | Description                             |
| ----------------------- | --------------------------------------- |
| **ML Definition**       | Teaches machines to learn from examples |
| **Supervised Learning** | Trains using labeled data               |
| **Linear Regression**   | Predicts continuous values              |
| **Loss Function**       | Measures prediction error               |
| **Goal of Training**    | Minimize loss â†’ improve accuracy        |
| **Inference**           | Use trained model for new predictions   |

---

## ğŸ§  Key Takeaways

* ML enables **learning from data** instead of hardcoded rules.
* **Supervised learning** focuses on mapping known inputs to outputs.
* **Linear regression** helps in predicting continuous outcomes (like price, temperature, sales).
* The **training process** adjusts weights and bias to minimize prediction errors.
* Once trained, the model can **predict unseen data accurately**.

---

---

## ğŸ§  Module: Supervised Learning â€“ Classification & Logistic Regression

**Source:** OCI Foundations Course â€“ Machine Learning Foundations
**Concept Summary Notes**

---

### ğŸ¯ What Youâ€™ll Learn

* Understand how **classification** works in supervised learning
* Learn the concept of **logistic regression** and how itâ€™s used
* Explore **binary and multi-class classification** problems
* Introduction to **Anaconda** and **Jupyter Notebook** setup for ML demos

---

### ğŸ“˜ What is Classification?

* In **supervised learning**, the model learns from **labeled data** (inputâ€“output pairs).
* If the **output** is:

  * **Continuous â†’ Regression**
  * **Categorical â†’ Classification**

**Definition:**

> Classification is a supervised ML technique used to categorize or assign data points into predefined classes based on their features or attributes.

**Examples:**

* **Binary Classification:** Spam detection â†’ (Spam / Not Spam)
* **Multi-class Classification:** Sentiment Analysis â†’ (Positive / Negative / Neutral)

---

### âš™ï¸ Logistic Regression â€” The Core Classifier

* A simple yet powerful algorithm for **binary classification** problems.
* Predicts probabilities of outcomes (True/False or Yes/No).
* Uses an **S-shaped curve** called the **sigmoid function** to map inputs between 0 and 1.

#### ğŸ§© Example: Pass or Fail Prediction

| Feature (Input) | Output (Label) |
| --------------- | -------------- |
| Hours of Study  | Pass / Fail    |

* The **sigmoid function** outputs a probability between 0 and 1.
* If probability > 0.5 â†’ **Pass**
  If probability < 0.5 â†’ **Fail**

**Sigmoid Function:**
[
f(x) = \frac{1}{1 + e^{-x}}
]

* Example:

  * Student studies **6 hours â†’ 0.8 probability â†’ Pass**
  * Student studies **4 hours â†’ 0.2 probability â†’ Fail**

---

### ğŸŒº Multi-Class Classification Example â€“ Iris Dataset

**Dataset:** *Iris Data*
Contains 150 samples from 3 flower species:

* *Iris Setosa*
* *Iris Versicolor*
* *Iris Virginica*

**Features (Inputs):**

* Sepal Length
* Sepal Width
* Petal Length
* Petal Width

**Output (Label):** Flower Type (3 classes)

â¡ï¸ Logistic Regression is extended to handle **multi-class classification** using this dataset.

---

## ğŸ§° Tool Setup: Anaconda & Jupyter Notebook

### ğŸ§± What is Anaconda?

> Anaconda is an open-source distribution for Python and R that simplifies **data science and machine learning** environment setup.

#### ğŸ”§ Key Features:

1. **Package Management:**
   Easily install, update, and manage Python libraries (like NumPy, Pandas, Scikit-learn).
2. **Environment Management:**
   Create isolated project environments to avoid dependency conflicts.
3. **User-Friendly Interface (Anaconda Navigator):**
   Manage environments, install packages, and launch tools like Jupyter â€” all visually.
4. **Cross-Platform:**
   Works on Windows, macOS, and Linux.

---

### ğŸ§® Jupyter Notebook Overview

> Jupyter Notebook is an interactive development environment (IDE) for writing, visualizing, and documenting live code.

#### âœ¨ Key Uses:

* Run live Python code
* Display data visualizations
* Combine code + equations + notes in one place
* Great for **data exploration** and **ML prototyping**

---

### ğŸ§‘â€ğŸ’» Hands-On Setup

1. **Launch Jupyter Notebook**
   Opens in your browser with tabs:

   * **Files:** View all notebooks/folders
   * **Running:** See active notebooks
   * **Clusters:** For parallel processing (advanced)

2. **Create a New Notebook**

   * Click â€œNew â†’ Python 3â€
   * Rename to `ML_Demo_1.ipynb`

3. **Run Your First Python Program**

   ```python
   # This program adds two numbers
   num1 = 1
   num2 = 4

   # Add two numbers
   sum = num1 + num2

   # Display result
   print("The sum of {} and {} is {}".format(num1, num2, sum))
   ```

   **Output:**

   ```
   The sum of 1 and 4 is 5
   ```

   âœ… Use **Shift + Enter** to run each cell.

---

### ğŸ§¾ Summary

* **Classification** assigns categories to data (binary or multi-class).
* **Logistic Regression** uses the **sigmoid function** to predict probabilities.
* The **Iris dataset** is a classic multi-class classification example.
* **Anaconda** + **Jupyter Notebook** provide an ideal setup for ML demos.

---

---

## ğŸ§  Module: Machine Learning Workflow â€“ Logistic Regression (Full Implementation)

**Source:** OCI Foundations Course â€“ Machine Learning Foundations
**Concept Summary Notes**

---

### ğŸ¯ What Youâ€™ll Learn

* Understand the **typical machine learning workflow**
* Implement **logistic regression classification** using the Iris dataset
* Learn about **data preprocessing, training, testing, evaluation, and prediction**
* Use **Anaconda + Jupyter Notebook** for practical ML development

---

## âš™ï¸ Machine Learning Workflow Overview

A typical **machine learning process** involves the following stages:

1. **ğŸ“¥ Load Data** â€“ Import and read data into a DataFrame.
2. **ğŸ§¹ Preprocess Data** â€“ Clean, normalize, and split data into features and labels.
3. **ğŸ§  Train Model** â€“ Fit the machine learning model on training data.
4. **ğŸ“Š Evaluate Model** â€“ Test the modelâ€™s performance on unseen data.
5. **ğŸ”® Make Predictions** â€“ Use the trained model to predict outcomes.

---

## ğŸ§© Part 1: Implementing Logistic Regression (Basic Demo)

### Step 1ï¸âƒ£ â€” Import Necessary Libraries

```python
# Import necessary libraries
import pandas as pd
from sklearn.linear_model import LogisticRegression
```

* **`pandas`**: For data manipulation (DataFrames & Series).
* **`sklearn.linear_model`**: Provides the `LogisticRegression` classifier.

> ğŸ’¡ If these libraries are not available, install them via terminal:

```bash
conda install -c anaconda scikit-learn
```

---

### Step 2ï¸âƒ£ â€” Load the Dataset

```python
# Load the dataset
iris_data = pd.read_csv("iris.csv")

# Display the first few rows
iris_data.head()
```

**Dataset:** `iris.csv`
Contains attributes of 3 Iris flower species â€” *Setosa, Versicolor, Virginica*.

| ID | SepalLength | SepalWidth | PetalLength | PetalWidth | Species     |
| -- | ----------- | ---------- | ----------- | ---------- | ----------- |
| 1  | 5.1         | 3.5        | 1.4         | 0.2        | Iris-setosa |

> `head()` is used to preview and understand data structure.

---

### Step 3ï¸âƒ£ â€” Split Data into Features & Labels

```python
# Separate features (X) and labels (y)
X = iris_data.drop(['ID', 'Species'], axis=1)
y = iris_data['Species']
```

* **Features (X):** Sepal & petal measurements
* **Labels (y):** Species name
* **Dropped:** `ID` column â€” not useful for training

---

### Step 4ï¸âƒ£ â€” Create & Train the Model

```python
# Initialize logistic regression model
model = LogisticRegression()

# Train the model
model.fit(X, y)
```

> The model learns relationships between input features and the target label (species).

---

### Step 5ï¸âƒ£ â€” Make Predictions

```python
# Predict species for new data points
new_data = [[5.1, 3.5, 1.4, 0.2]]
prediction = model.predict(new_data)

print("Predicted Species:", prediction)
```

**Output:**

```
Predicted Species: ['Iris-setosa']
```

âœ… **Workflow Summary:**

* Loaded dataset
* Split into X and y
* Trained logistic regression model
* Made and printed predictions

---

## ğŸ§© Part 2: Extended Demo â€“ Full Workflow (With Preprocessing & Evaluation)

### Step 1ï¸âƒ£ â€” Setup New Notebook

* Duplicate your existing notebook â†’ rename as **`MLDemo2.ipynb`**
* Restart the kernel and **clear outputs**
* Import additional libraries:

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
```

---

### Step 2ï¸âƒ£ â€” Understand the New Libraries

| Library              | Purpose                                    |
| -------------------- | ------------------------------------------ |
| **NumPy (`np`)**     | Efficient numerical computations           |
| **train_test_split** | Split dataset into training & test subsets |
| **StandardScaler**   | Normalize features (mean = 0, std = 1)     |
| **accuracy_score**   | Measure model performance                  |

---

### ğŸ§® Why Standardization Matters

* Features with large numeric ranges (e.g., 1000â€“5000 sqft) can dominate learning.
* Standardization ensures all features contribute **equally** to model training.

**Formula:**
[
z = \frac{(x - \mu)}{\sigma}
]

> Makes every feature centered around 0 and scaled by its variance.

---

### ğŸ§ª Step 3ï¸âƒ£ â€” Split Dataset

```python
# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

* **`test_size=0.2`** â†’ 20% data used for testing
* **`random_state=42`** â†’ ensures reproducibility

---

### âš–ï¸ Step 4ï¸âƒ£ â€” Standardize Features

```python
# Initialize StandardScaler
scaler = StandardScaler()

# Fit on training data & transform both sets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

---

### ğŸ§  Step 5ï¸âƒ£ â€” Train Logistic Regression Model

```python
# Initialize and train model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)
```

---

### ğŸ“ˆ Step 6ï¸âƒ£ â€” Evaluate Model Performance

```python
# Make predictions on test data
y_pred = model.predict(X_test_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy:", accuracy)
```

**Output:**

```
Model Accuracy: 1.0
```

âœ… Perfect accuracy (100%) â€” model predicted all test samples correctly.

---

### ğŸ”® Step 7ï¸âƒ£ â€” Predict on New Data

```python
# Sample new data (sepal & petal features)
new_data = np.array([
    [5.1, 3.5, 1.4, 0.2],
    [6.2, 3.4, 5.4, 2.3],
    [5.9, 3.0, 5.1, 1.8]
])

# Standardize new data
new_data_scaled = scaler.transform(new_data)

# Make predictions
new_predictions = model.predict(new_data_scaled)
print("Predicted Classes:", new_predictions)
```

**Output:**

```
Predicted Classes: ['Iris-setosa' 'Iris-virginica' 'Iris-setosa']
```

---

## âœ… Full Workflow Summary

| Step                 | Description                                    |
| -------------------- | ---------------------------------------------- |
| 1ï¸âƒ£ Import Libraries | Loaded Pandas, NumPy, and Scikit-learn modules |
| 2ï¸âƒ£ Load Dataset     | Read and previewed the Iris dataset            |
| 3ï¸âƒ£ Split Data       | Separated features (X) and labels (y)          |
| 4ï¸âƒ£ Train/Test Split | Divided data into training and testing sets    |
| 5ï¸âƒ£ Standardization  | Normalized data for balanced feature scaling   |
| 6ï¸âƒ£ Model Training   | Trained Logistic Regression on scaled data     |
| 7ï¸âƒ£ Evaluation       | Achieved 100% accuracy on test set             |
| 8ï¸âƒ£ Prediction       | Made predictions on unseen flower data         |

---

### ğŸ’¡ Key Takeaways

* **Logistic Regression** is powerful for both **binary** and **multi-class** classification.
* Always **standardize** features to improve model performance.
* Use **train/test split** for fair model evaluation.
* **Accuracy score** helps measure how well your model generalizes.
* Jupyter Notebook provides an excellent interactive coding environment for ML projects.

---

---

# ğŸ§© Lesson: Unsupervised Learning & Reinforcement Learning

*(Oracle AI Foundations â€” Structured Notes)*

---

## ğŸ§  Part 1: Unsupervised Machine Learning

### ğŸ” What is Unsupervised Learning?

Unsupervised learning is a type of machine learning where **no labeled outputs** are provided.
The algorithm **learns patterns and relationships** in data to group similar data points together.

#### ğŸ¨ Example Analogy

* Imagine giving a child a box of LEGO pieces and asking them to group them.

  * They might group by **color**, **size**, or **shape** â€” based on patterns they observe.
* Similarly, an algorithm groups **unlabeled data** into clusters.

#### ğŸ Example: Fruits Basket

You have apples, bananas, and oranges.

* The algorithm groups:

  * Round, red fruits (apples) â†’ one cluster
  * Long, yellow fruits (bananas) â†’ another cluster
* Without being told explicitly â€” the algorithm discovers these groupings on its own.

---

### ğŸ“Š Clustering Concept

Clustering groups similar data points into clusters.

* Inside a cluster â†’ data points are **more similar** to each other.
* Outside a cluster â†’ data points are **less similar**.
* Points that donâ€™t belong anywhere â†’ **outliers**.

**Example:** Grapes differ from apples and pears â†’ identified as an **outlier**.

---

### ğŸ’¼ Common Use Cases of Unsupervised Learning

| Use Case                         | Description                                   | Example                                                            |
| -------------------------------- | --------------------------------------------- | ------------------------------------------------------------------ |
| ğŸ› **Market Segmentation**       | Grouping customers based on purchase behavior | Customers buying protein-rich products â†’ show them sports ads      |
| ğŸ’³ **Outlier / Fraud Detection** | Detecting unusual patterns                    | Banks detect fraud from unusually high or recurring transactions   |
| ğŸ¬ **Recommendation Systems**    | Suggesting content based on preferences       | Netflix groups users by movie types â†’ personalized recommendations |

---

### ğŸ“ Understanding Similarity

Similarity = how **close** two data points are to each other (value between **0 and 1**).

* Higher value â†’ more similar.
* Example: Apple ğŸ and Cherry ğŸ’ â†’ high similarity (closer to 1) because both are red and round.

---

### âš™ï¸ Workflow of Unsupervised Learning

| Step                              | Description                                                                                                                    |
| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| **1. Prepare Data**               | Clean data â†’ remove missing values, normalize features                                                                         |
| **2. Create Similarity Matrix**   | Calculate distances/similarities between data points                                                                           |
| **3. Choose Similarity Metric**   | Common metrics: <br> - Euclidean Distance <br> - Manhattan Distance <br> - Cosine Similarity <br> - Jaccard Similarity         |
| **4. Run Clustering Algorithm**   | Algorithms: <br> - Partition-based (K-Means) <br> - Hierarchical <br> - Density-based (DBSCAN) <br> - Distribution-based (GMM) |
| **5. Interpret & Refine Results** | Check quality â†’ compare against expectations <br> Adjust preprocessing or parameters iteratively                               |

---

## ğŸ¤– Part 2: Reinforcement Learning

### ğŸ¦´ What is Reinforcement Learning?

Reinforcement Learning (RL) is like **training a dog with rewards and penalties**.

* The agent learns **by interacting with the environment**.
* It receives **rewards** or **penalties** based on its actions.
* No labeled data is used.

---

### ğŸ’¡ Real-World Applications

| Domain                        | Example                                                                  |
| ----------------------------- | ------------------------------------------------------------------------ |
| ğŸš— **Autonomous Vehicles**    | Self-driving cars learn to steer using camera input and traffic feedback |
| ğŸ  **Smart Home Devices**     | Alexa/Siri/Google Assistant adapt to usersâ€™ speech & preferences         |
| ğŸ­ **Industrial Automation**  | Robots learn optimal control for production and efficiency               |
| ğŸ® **Gaming & Entertainment** | AI opponents learn from player actions to improve gameplay               |

---

### ğŸ§© Key Reinforcement Learning Concepts

| Term            | Definition                           | Example                                   |
| --------------- | ------------------------------------ | ----------------------------------------- |
| **Agent**       | Learner or decision-maker            | The carâ€™s driving AI system               |
| **Environment** | The world the agent interacts with   | The road and surroundings                 |
| **State**       | Current situation of the environment | Camera view of the road                   |
| **Action**      | Move or decision the agent can take  | Turn left, right, or go straight          |
| **Reward**      | Feedback signal for an action        | +1 for staying on track, -1 for drifting  |
| **Policy**      | Strategy that maps states â†’ actions  | â€œIf curve detected â†’ turn right slightlyâ€ |

---

### ğŸ¶ Analogy: Training a Dog

* Dog = **Agent**
* Training ground = **Environment**
* Commands = **Actions**
* Rewards for obeying = **Positive Reward**
* Scolding for mistakes = **Penalty**

Over time, the dog learns a **policy** â€” what to do to maximize rewards.

---

### ğŸ§­ Goal of Reinforcement Learning

The goal is to find the **Optimal Policy** â€”
a strategy that yields **maximum cumulative rewards** for the agent.

**The agent learns through trial and error**, using algorithms like:

* **Q-Learning**
* **Deep Q-Learning (DQN)**

---

### âš™ï¸ Example: Robotic Arm in Warehouse

Goal â†’ Teach a robotic arm to pick and place goods efficiently.

**Steps:**

1. **Define Environment:**
   The warehouse, goods, and target shelves.
2. **Define States:**
   Armâ€™s position, object position, and target location.
3. **Define Actions:**
   Move arm left/right/up/down, pick up, place.
4. **Define Rewards:**
   âœ… Reward for placing correctly
   âŒ Penalty for dropping/damaging
5. **Training Process:**

   * Start randomly â†’ explore various actions
   * Observe rewards/penalties
   * Gradually learn which actions maximize rewards
   * Result â†’ Optimized strategy (optimal policy)

---

### ğŸ Summary

| Concept       | Supervised           | Unsupervised           | Reinforcement    |
| ------------- | -------------------- | ---------------------- | ---------------- |
| **Data Type** | Labeled              | Unlabeled              | Feedback-based   |
| **Goal**      | Predict known labels | Find patterns/clusters | Maximize reward  |
| **Example**   | Spam detection       | Customer segmentation  | Self-driving car |

---

### ğŸ“˜ Key Takeaways

* **Unsupervised Learning** â†’ discovers patterns or clusters without labels.
* **Reinforcement Learning** â†’ learns from interaction and feedback.
* Both are critical for **AI automation and intelligent systems**.

---
